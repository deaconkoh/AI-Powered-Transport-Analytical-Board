# -*- coding: utf-8 -*-
"""cloud_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d4XD2Ev_JWrA7U9tqRVonIY82sOz8TCg
"""

import os
import json
from datetime import datetime

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import glob
import joblib
import xgboost as xgb
import gc

warnings.filterwarnings("ignore")

def json_default(o):
    import numpy as np

    # Scalars
    if isinstance(o, (np.floating,)):
        return float(o)
    if isinstance(o, (np.integer,)):
        return int(o)
    if isinstance(o, (np.bool_,)):
        return bool(o)

    # Arrays
    if isinstance(o, np.ndarray):
        return o.tolist()

    # Fallback ‚Äì stringify anything else
    return str(o)


# --------- PATH CONFIG (works locally AND on AWS) ---------
# Base path (can be overridden by env var)
BASE_PATH = os.environ.get("DRIVE_PATH", ".")

# Where gold parquet files live
GOLD_DATA_PATH = os.environ.get(
    "GOLD_DATA_PATH",
    os.path.join(BASE_PATH, "gold_data")
)

# Where models/checkpoints/reports will be saved
MODEL_SAVE_PATH = os.environ.get(
    "MODEL_SAVE_PATH",
    os.path.join(BASE_PATH, "traffic_models_hybrid")
)
os.makedirs(MODEL_SAVE_PATH, exist_ok=True)

print("üöÄ Starting LSTM + XGBoost Hybrid Training - ALL FILES")
print("   GOLD_DATA_PATH  =", GOLD_DATA_PATH)
print("   MODEL_SAVE_PATH =", MODEL_SAVE_PATH)
# ----------------------------------------------------------

class HybridLSTM(nn.Module):
    """LSTM model for temporal patterns"""
    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.2):
        super(HybridLSTM, self).__init__()

        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )

        self.attention = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.Tanh(),
            nn.Linear(hidden_size, 1)
        )

        self.output = nn.Linear(hidden_size, 1)

    def forward(self, x):
        lstm_out, (hidden, cell) = self.lstm(x)

        # Attention mechanism
        attention_weights = torch.softmax(self.attention(lstm_out), dim=1)
        context_vector = torch.sum(attention_weights * lstm_out, dim=1)

        output = self.output(context_vector)
        return output.squeeze()

class HybridTrainer:
    """Trainer that combines LSTM and XGBoost with incremental training"""

    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.lstm_model = None
        self.xgb_model = None
        self.scaler = StandardScaler()
        self.file_training_history = []

        # Optimized Configuration for ALL files
        self.config = {
            'lstm': {
                'batch_size': 128,  # Increased for efficiency
                'learning_rate': 0.001,
                'hidden_size': 64,
                'num_layers': 2,
                'sequence_length': 6,
                'target_chunks_per_file': 200,  # Balanced number
                'early_stopping_patience': 30,
                'max_rows_per_file': 1000000,   # 1M rows max per file
                'min_chunk_size': 10000,        # Larger chunks for efficiency
                'max_chunk_size': 100000,
                'min_chunks_for_training': 10   # Minimum chunks to consider file useful
            },
            'xgb': {
                'n_estimators': 100,
                'learning_rate': 0.1,
                'max_depth': 6,
                'subsample': 0.8
            },
            'training': {
                'max_files': None,  # Process all files
                'resume_from_checkpoint': True,
                'save_every_n_files': 2,  # Save checkpoint every 2 files
                'memory_cleanup_every': 5  # Clean memory every 5 files
            }
        }

        print(f"üéØ Hybrid Trainer Initialized - ALL FILES MODE")
        print(f"üìä Device: {self.device}")
        print(f"‚ö° Optimized for processing ALL gold files")

    def get_file_paths(self):
        """Get list of all data files"""
        file_paths = glob.glob(os.path.join(GOLD_DATA_PATH, "*.parquet"))
        file_paths = [f for f in file_paths if not f.endswith('_temp.parquet')]
        file_paths.sort()  # Sort for consistent order
        print(f"üìÅ Found {len(file_paths)} gold files")
        return file_paths

    def find_latest_checkpoint(self):
        """Find the latest checkpoint to resume from"""
        checkpoint_files = glob.glob(os.path.join(MODEL_SAVE_PATH, "hybrid_checkpoint_file_*.pth"))
        if not checkpoint_files:
            return None

        # Extract file numbers and find the latest
        file_numbers = []
        for file_path in checkpoint_files:
            try:
                # Extract number from "hybrid_checkpoint_file_X.pth"
                file_name = os.path.basename(file_path)
                number = int(file_name.split('_')[-1].split('.')[0])
                file_numbers.append((number, file_path))
            except:
                continue

        if not file_numbers:
            return None

        # Return the latest checkpoint
        latest_number, latest_path = max(file_numbers, key=lambda x: x[0])
        print(f"üîÑ Found checkpoint for file {latest_number}: {latest_path}")
        return latest_number, latest_path

    def load_checkpoint(self, checkpoint_path):
        """Load training checkpoint"""
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device)

            # Load LSTM model
            if 'lstm_state_dict' in checkpoint and checkpoint['lstm_state_dict'] is not None:
                if self.lstm_model is None:
                    # Initialize model with correct input size (will be set during first training)
                    pass
                self.lstm_model.load_state_dict(checkpoint['lstm_state_dict'])
                self.lstm_optimizer.load_state_dict(checkpoint['lstm_optimizer_state_dict'])
                print("   ‚úÖ LSTM model loaded from checkpoint")

            # Load training history
            if 'training_history' in checkpoint:
                self.file_training_history = checkpoint['training_history']
                print(f"   ‚úÖ Training history loaded ({len(self.file_training_history)} files)")

            # Load XGBoost model if available
            if 'xgb_model_path' in checkpoint and os.path.exists(checkpoint['xgb_model_path']):
                self.xgb_model = xgb.XGBRegressor()
                self.xgb_model.load_model(checkpoint['xgb_model_path'])
                print("   ‚úÖ XGBoost model loaded from checkpoint")

            # Load scaler if available
            if 'scaler_path' in checkpoint and os.path.exists(checkpoint['scaler_path']):
                self.xgb_scaler = joblib.load(checkpoint['scaler_path'])
                print("   ‚úÖ Scaler loaded from checkpoint")

            return True

        except Exception as e:
            print(f"   ‚ùå Error loading checkpoint: {e}")
            return False

    def analyze_file_columns(self, file_path):
        """Analyze what columns are available in a file"""
        try:
            # Read only first 1000 rows for analysis to save time
            sample = pd.read_parquet(file_path)
            num_rows = len(sample)

            numeric_cols = sample.select_dtypes(include=[np.number]).columns.tolist()

            # Remove potential target columns
            feature_cols = [col for col in numeric_cols
                          if not any(x in col.lower() for x in ['target', 'future', 'label'])]

            # Clean up
            del sample
            gc.collect()

            return feature_cols, num_rows

        except Exception as e:
            print(f"‚ö†Ô∏è Error analyzing {file_path}: {e}")
            return ['AverageSpeed', 'hour', 'day_of_week'], 0

    def sample_data_strategically(self, df, target_rows=1000000):
        """Sample data strategically to maintain temporal patterns"""
        if len(df) <= target_rows:
            return df

        original_size = len(df)
        # For very large datasets, use systematic sampling
        if len(df) > 2_000_000:
            step = max(2, len(df) // target_rows)  # Minimum step of 2
            sampled_df = df.iloc[::step].copy()
            print(f"   üìâ Systematic sampling: {len(sampled_df):,} rows from {original_size:,} (step: {step})")
        else:
            # For moderately large datasets, take first N rows
            sampled_df = df.head(target_rows).copy()
            print(f"   üìâ First {len(sampled_df):,} rows from {original_size:,}")

        return sampled_df

    def calculate_optimal_chunk_size(self, num_rows, target_chunks=200):
        """Calculate chunk size to achieve target number of chunks"""
        chunk_size = max(
            self.config['lstm']['min_chunk_size'],
            min(
                num_rows // target_chunks,
                self.config['lstm']['max_chunk_size']
            )
        )
        return chunk_size

    def should_early_stop(self, current_loss, previous_losses, patience=30):
        """Check if training should stop early"""
        if len(previous_losses) < patience:
            return False

        # Check if loss has plateaued
        recent_losses = previous_losses[-patience:]
        avg_recent = np.mean(recent_losses)

        # Stop if improvement less than 0.2% in last 'patience' chunks
        improvement = (avg_recent - current_loss) / avg_recent if avg_recent > 0 else 1.0
        return improvement < 0.002

    def prepare_hybrid_features(self, df, feature_cols):
        """Prepare features for both LSTM and XGBoost"""
        try:
            # Use only available columns
            available_cols = [col for col in feature_cols if col in df.columns]

            if len(available_cols) == 0:
                # Fallback to basic columns
                available_cols = [col for col in df.columns if df[col].dtype in [np.int64, np.float64]]
                available_cols = available_cols[:4]  # Use first 4 numeric columns

            if len(available_cols) == 0:
                return None, None, 0

            features = df[available_cols].values.astype(np.float32)

            # Create target (predict next value of first feature)
            if len(features) > self.config['lstm']['sequence_length']:
                targets = features[self.config['lstm']['sequence_length']:, 0]  # Use first column as target
                features = features[:len(targets)]  # Align features
            else:
                targets = features[:, 0]  # Fallback

            return features, targets, len(available_cols)

        except Exception as e:
            print(f"‚ö†Ô∏è Feature preparation error: {e}")
            return None, None, 0

    def create_sequences(self, features, targets):
        """Create sequences for LSTM training"""
        sequence_length = self.config['lstm']['sequence_length']
        sequences = []
        sequence_targets = []

        for i in range(len(features) - sequence_length):
            sequences.append(features[i:i+sequence_length])
            sequence_targets.append(targets[i])

        return np.array(sequences), np.array(sequence_targets)

    def train_on_single_file(self, file_path, file_index, total_files):
        """Train both LSTM and XGBoost on a single file"""
        print(f"\nüìÅ Processing File {file_index+1}/{total_files}: {os.path.basename(file_path)}")

        try:
            # Analyze file
            feature_cols, num_rows = self.analyze_file_columns(file_path)
            print(f"   üìä Rows: {num_rows:,}, Features: {len(feature_cols)}")

            if num_rows < self.config['lstm']['min_chunks_for_training'] * self.config['lstm']['min_chunk_size']:
                print(f"   ‚ö†Ô∏è File too small, skipping (minimum {self.config['lstm']['min_chunks_for_training'] * self.config['lstm']['min_chunk_size']} rows required)")
                return 0, 0

            # Read file
            print("   üìñ Reading file...")
            df = pd.read_parquet(file_path)

            # Sample data for very large files
            if len(df) > self.config['lstm']['max_rows_per_file']:
                df = self.sample_data_strategically(df, self.config['lstm']['max_rows_per_file'])
                num_rows = len(df)
                print(f"   ‚úÖ Using sampled data: {num_rows:,} rows")

            # Calculate optimal chunk size
            chunk_size = self.calculate_optimal_chunk_size(num_rows, self.config['lstm']['target_chunks_per_file'])
            target_chunks = self.config['lstm']['target_chunks_per_file']

            actual_chunks = min(target_chunks * 2, (num_rows + chunk_size - 1) // chunk_size)
            print(f"   ‚ö° Chunk size: {chunk_size:,}, Target: {target_chunks}, Actual: {actual_chunks}")

            lstm_losses = []
            xgb_data = []

            print(f"   üéØ Processing {actual_chunks} chunks...")

            chunks_processed = 0
            for chunk_idx in range(0, min(len(df), chunk_size * actual_chunks), chunk_size):
                chunk_end = min(chunk_idx + chunk_size, len(df))
                chunk = df.iloc[chunk_idx:chunk_end]

                current_chunk_num = (chunk_idx // chunk_size) + 1
                chunks_processed += 1

                # Print progress every 20 chunks
                if current_chunk_num % 20 == 0 or current_chunk_num == 1:
                    print(f"   üîÑ Chunk {current_chunk_num}/{actual_chunks}...", end=" ")

                # Prepare features
                features, targets, input_size = self.prepare_hybrid_features(chunk, feature_cols)

                if features is None or len(features) < self.config['lstm']['sequence_length'] + 1:
                    if current_chunk_num % 20 == 0:
                        print("Skipped (insufficient data)")
                    continue

                # Initialize LSTM model if first time
                if self.lstm_model is None:
                    self.lstm_model = HybridLSTM(
                        input_size=input_size,
                        hidden_size=self.config['lstm']['hidden_size'],
                        num_layers=self.config['lstm']['num_layers']
                    ).to(self.device)
                    self.lstm_optimizer = optim.Adam(
                        self.lstm_model.parameters(),
                        lr=self.config['lstm']['learning_rate']
                    )
                    self.lstm_criterion = nn.HuberLoss()
                    print(f"üéØ LSTM initialized with input size: {input_size}")

                # Train LSTM
                lstm_loss = self.train_lstm_chunk(features, targets)
                lstm_losses.append(lstm_loss)

                # Check for early stopping
                if len(lstm_losses) > self.config['lstm']['early_stopping_patience']:
                    if self.should_early_stop(lstm_loss, lstm_losses,
                                            self.config['lstm']['early_stopping_patience']):
                        print(f"üõë Early stopping at chunk {current_chunk_num} (loss plateau)")
                        break

                # Stop if we've reached target chunks and loss is reasonable
                if (current_chunk_num >= target_chunks and
                    len(lstm_losses) > 50 and
                    np.mean(lstm_losses[-20:]) < 1.0):
                    print(f"‚úÖ Target reached at chunk {current_chunk_num} with good loss")
                    break

                # Prepare XGBoost data for most chunks
                if current_chunk_num <= actual_chunks * 0.8:  # Use 80% of chunks for XGB
                    xgb_features, xgb_targets = self.prepare_xgb_data(features, targets)
                    if xgb_features is not None and len(xgb_features) > 0:
                        xgb_data.append((xgb_features, xgb_targets))

                if current_chunk_num % 20 == 0 or current_chunk_num == 1:
                    print(f"Loss: {lstm_loss:.4f}")

                # Clean memory
                del features, targets, chunk
                if current_chunk_num % 50 == 0:  # GC every 50 chunks
                    gc.collect()

            # Clean up the main dataframe
            del df
            gc.collect()

            # Train XGBoost on collected data
            xgb_loss = 0
            if xgb_data and len(xgb_data) > 5:  # Only train if we have enough data
                try:
                    print("   üå≥ Training XGBoost on collected data...")
                    xgb_features_all = np.vstack([data[0] for data in xgb_data])
                    xgb_targets_all = np.concatenate([data[1] for data in xgb_data])

                    # Limit XGB data to prevent memory issues
                    if len(xgb_features_all) > 100000:
                        indices = np.random.choice(len(xgb_features_all), 100000, replace=False)
                        xgb_features_all = xgb_features_all[indices]
                        xgb_targets_all = xgb_targets_all[indices]
                        print(f"   üìâ Sampled XGBoost data to 100,000 rows")

                    xgb_loss = self.train_xgb_chunk(xgb_features_all, xgb_targets_all)
                    print(f"   üå≥ XGBoost Loss: {xgb_loss:.4f}")
                except Exception as e:
                    print(f"   ‚ö†Ô∏è XGBoost training skipped: {e}")
                    xgb_loss = 0
            else:
                print(f"   üå≥ XGBoost: Insufficient data ({len(xgb_data) if xgb_data else 0} chunks)")

            # Record training history
            avg_lstm_loss = np.mean(lstm_losses) if lstm_losses else 0
            self.file_training_history.append({
                'file': os.path.basename(file_path),
                'file_index': file_index,
                'lstm_loss': avg_lstm_loss,
                'xgb_loss': xgb_loss,
                'rows_processed': num_rows,
                'chunks_used': len(lstm_losses),
                'timestamp': datetime.now().isoformat()
            })

            return avg_lstm_loss, xgb_loss

        except Exception as e:
            print(f"   ‚ùå File training error: {e}")
            import traceback
            traceback.print_exc()
            return 0, 0

    def train_lstm_chunk(self, features, targets):
        """Train LSTM on a single chunk"""
        sequences, sequence_targets = self.create_sequences(features, targets)

        if len(sequences) == 0:
            return 0

        # Convert to tensors
        sequences_tensor = torch.FloatTensor(sequences).to(self.device)
        targets_tensor = torch.FloatTensor(sequence_targets).to(self.device)

        # Create DataLoader for batch training
        dataset = torch.utils.data.TensorDataset(sequences_tensor, targets_tensor)
        dataloader = torch.utils.data.DataLoader(
            dataset,
            batch_size=self.config['lstm']['batch_size'],
            shuffle=True
        )

        # Train
        self.lstm_model.train()
        total_loss = 0
        batches = 0

        for batch_sequences, batch_targets in dataloader:
            self.lstm_optimizer.zero_grad()

            predictions = self.lstm_model(batch_sequences)
            loss = self.lstm_criterion(predictions, batch_targets)
            loss.backward()

            # Gradient clipping to prevent explosions
            torch.nn.utils.clip_grad_norm_(self.lstm_model.parameters(), max_norm=1.0)

            self.lstm_optimizer.step()

            total_loss += loss.item()
            batches += 1

        return total_loss / batches if batches > 0 else 0

    def prepare_xgb_data(self, features, targets):
        """Prepare data for XGBoost training"""
        try:
            # Use LSTM predictions as additional features
            if self.lstm_model is not None and len(features) >= self.config['lstm']['sequence_length']:
                self.lstm_model.eval()
                with torch.no_grad():
                    sequences, _ = self.create_sequences(features, targets)
                    if len(sequences) > 0:
                        sequences_tensor = torch.FloatTensor(sequences).to(self.device)
                        lstm_predictions = self.lstm_model(sequences_tensor).cpu().numpy()

                        # Combine original features with LSTM predictions
                        original_features = features[self.config['lstm']['sequence_length']:]
                        if len(original_features) == len(lstm_predictions):
                            # Use last time step features + LSTM predictions
                            xgb_features = np.column_stack([
                                original_features[:, :min(4, original_features.shape[1])],  # First 4 features
                                lstm_predictions.reshape(-1, 1)  # LSTM predictions
                            ])
                            xgb_targets = targets[:len(xgb_features)]
                            return xgb_features, xgb_targets

            # Fallback: use just the original features
            if len(features) > 0:
                xgb_features = features[:min(len(features), len(targets)), :min(4, features.shape[1])]
                xgb_targets = targets[:len(xgb_features)]
                return xgb_features, xgb_targets

            return np.array([]), np.array([])

        except Exception as e:
            print(f"‚ö†Ô∏è XGB data prep error: {e}")
            return np.array([]), np.array([])

    def train_xgb_chunk(self, features, targets):
        """Train XGBoost on a chunk of data"""
        if len(features) == 0 or len(targets) == 0:
            return 0

        try:
            # Initialize XGBoost if first time
            if self.xgb_model is None:
                self.xgb_model = xgb.XGBRegressor(**self.config['xgb'])

            # Scale features
            if hasattr(self, 'xgb_scaler'):
                features_scaled = self.xgb_scaler.transform(features)
            else:
                self.xgb_scaler = StandardScaler()
                features_scaled = self.xgb_scaler.fit_transform(features)

            # Train or update XGBoost
            if not hasattr(self.xgb_model, 'is_fitted') or not self.xgb_model.is_fitted:
                self.xgb_model.fit(features_scaled, targets)
            else:
                # Continue training on new data
                self.xgb_model.fit(
                    features_scaled, targets,
                    xgb_model=self.xgb_model.get_booster()
                )

            # Calculate loss
            predictions = self.xgb_model.predict(features_scaled)
            loss = mean_absolute_error(targets, predictions)

            return loss

        except Exception as e:
            print(f"‚ö†Ô∏è XGB training error: {e}")
            return 0

    def run_hybrid_training(self, max_files=None):
        """Run hybrid training on ALL files"""
        file_paths = self.get_file_paths()
        if not file_paths:
            print("‚ùå No data files found")
            return

        # Check for existing checkpoint
        start_file_index = 0
        if self.config['training']['resume_from_checkpoint']:
            checkpoint_info = self.find_latest_checkpoint()
            if checkpoint_info:
                latest_file_number, checkpoint_path = checkpoint_info
                print(f"üîÑ Resuming from checkpoint at file {latest_file_number}")
                if self.load_checkpoint(checkpoint_path):
                    start_file_index = latest_file_number
                    print(f"üîÑ Resuming from file {start_file_index + 1}")

        if max_files is None:
            files_to_use = file_paths[start_file_index:]
        else:
            files_to_use = file_paths[start_file_index:start_file_index + max_files]

        print(f"üéØ Training on {len(files_to_use)} files (starting from file {start_file_index + 1})")
        print(f"‚ö° ALL FILES Configuration:")
        print(f"   - Max rows per file: {self.config['lstm']['max_rows_per_file']:,}")
        print(f"   - Target chunks per file: {self.config['lstm']['target_chunks_per_file']}")
        print(f"   - Save checkpoint every: {self.config['training']['save_every_n_files']} files")
        print(f"   - Memory cleanup every: {self.config['training']['memory_cleanup_every']} files")

        total_files = len(files_to_use)
        file_losses = []

        for relative_index, file_path in enumerate(files_to_use):
            file_index = start_file_index + relative_index

            print(f"\n{'='*60}")
            print(f"üìö FILE {file_index+1}/{len(file_paths)}")
            print(f"üìÅ {os.path.basename(file_path)}")
            print(f"{'='*60}")

            # Train on current file
            lstm_loss, xgb_loss = self.train_on_single_file(file_path, file_index, len(file_paths))
            file_losses.append((lstm_loss, xgb_loss))

            # Save checkpoint periodically
            if (file_index + 1) % self.config['training']['save_every_n_files'] == 0:
                self.save_hybrid_checkpoint(file_index + 1)

            # Memory cleanup periodically
            if (file_index + 1) % self.config['training']['memory_cleanup_every'] == 0:
                print("üßπ Performing memory cleanup...")
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

            # Print progress
            print(f"‚úÖ File {file_index+1} completed:")
            print(f"   ü§ñ LSTM Loss: {lstm_loss:.4f}")
            print(f"   üå≥ XGBoost Loss: {xgb_loss:.4f}")

            # Estimate remaining time
            if relative_index > 0:
                avg_time_per_file = (datetime.now() - start_time).total_seconds() / (relative_index + 1)
                remaining_files = total_files - (relative_index + 1)
                remaining_time = avg_time_per_file * remaining_files
                print(f"   ‚è±Ô∏è Estimated time remaining: {remaining_time/60:.1f} minutes")

        # Final evaluation
        self.final_evaluation(file_losses)

    def save_hybrid_checkpoint(self, file_number):
        """Save hybrid model checkpoint"""
        checkpoint = {
            'file_number': file_number,
            'timestamp': datetime.now().isoformat(),
            'training_history': self.file_training_history,
            'config': self.config
        }

        # Save LSTM model
        if self.lstm_model is not None:
            checkpoint['lstm_state_dict'] = self.lstm_model.state_dict()
            checkpoint['lstm_optimizer_state_dict'] = self.lstm_optimizer.state_dict()

        # Save XGBoost model
        if self.xgb_model is not None:
            xgb_path = os.path.join(MODEL_SAVE_PATH, f'xgb_model_file_{file_number}.json')
            self.xgb_model.save_model(xgb_path)
            checkpoint['xgb_model_path'] = xgb_path

        # Save scaler
        if hasattr(self, 'xgb_scaler'):
            scaler_path = os.path.join(MODEL_SAVE_PATH, f'scaler_file_{file_number}.joblib')
            joblib.dump(self.xgb_scaler, scaler_path)
            checkpoint['scaler_path'] = scaler_path

        # Save checkpoint
        checkpoint_path = os.path.join(MODEL_SAVE_PATH, f'hybrid_checkpoint_file_{file_number}.pth')
        torch.save(checkpoint, checkpoint_path)

        print(f"üíæ Checkpoint saved: {checkpoint_path}")

    def final_evaluation(self, file_losses):
        """Final evaluation and reporting"""
        print(f"\n{'='*60}")
        print(f"üéâ ALL FILES TRAINING COMPLETED!")
        print(f"{'='*60}")

        # Calculate average losses
        lstm_losses = [loss[0] for loss in file_losses if loss[0] > 0]
        xgb_losses = [loss[1] for loss in file_losses if loss[1] > 0]

        if lstm_losses:
            print(f"ü§ñ LSTM Final Average Loss: {np.mean(lstm_losses):.4f}")
            print(f"   Best LSTM Loss: {min(lstm_losses):.4f}")
            print(f"   Worst LSTM Loss: {max(lstm_losses):.4f}")
        if xgb_losses:
            print(f"üå≥ XGBoost Final Average Loss: {np.mean(xgb_losses):.4f}")
            print(f"   Best XGBoost Loss: {min(xgb_losses):.4f}")

        # Print training summary
        total_chunks = sum([entry.get('chunks_used', 0) for entry in self.file_training_history])
        total_rows = sum([entry.get('rows_processed', 0) for entry in self.file_training_history])
        avg_chunks_per_file = total_chunks / len(self.file_training_history) if self.file_training_history else 0

        print(f"üìà Training Summary:")
        print(f"   Total files processed: {len(self.file_training_history)}")
        print(f"   Total chunks used: {total_chunks}")
        print(f"   Average chunks per file: {avg_chunks_per_file:.1f}")
        print(f"   Total rows processed: {total_rows:,}")

        # Save final combined model
        self.save_final_hybrid_model()

        # Create training report
        self.save_training_report()

    def save_final_hybrid_model(self):
        """Save the final hybrid model"""
        if self.lstm_model is None:
            print("‚ùå No LSTM model to save")
            return

        final_checkpoint = {
            'lstm_state_dict': self.lstm_model.state_dict(),
            'config': self.config,
            'training_history': self.file_training_history,
            'final_timestamp': datetime.now().isoformat(),
            'model_type': 'LSTM_XGBoost_Hybrid'
        }

        final_path = os.path.join(MODEL_SAVE_PATH, 'final_hybrid_model.pth')
        torch.save(final_checkpoint, final_path)

        # Also save XGBoost separately
        if self.xgb_model is not None:
            xgb_final_path = os.path.join(MODEL_SAVE_PATH, 'final_xgb_model.json')
            self.xgb_model.save_model(xgb_final_path)
            print(f"üíæ Final XGBoost model saved: {xgb_final_path}")

        print(f"üíæ Final hybrid model saved: {final_path}")

    def save_training_report(self):
        """Save detailed training report"""
        report = {
            'training_summary': {
                'total_files': len(self.file_training_history),
                'final_timestamp': datetime.now().isoformat(),
                'device_used': str(self.device),
                'optimization_settings': self.config
            },
            'file_progress': self.file_training_history,
            'model_architecture': {
                'lstm': {
                    'hidden_size': self.config['lstm']['hidden_size'],
                    'num_layers': self.config['lstm']['num_layers'],
                    'sequence_length': self.config['lstm']['sequence_length']
                },
                'xgb': self.config['xgb']
            },
            'performance_notes': 'ALL FILES configuration with checkpoint resuming and memory optimization'
        }

        report_path = os.path.join(MODEL_SAVE_PATH, 'hybrid_training_report.json')
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=json_default)

        print(f"üìã Training report saved: {report_path}")

# Memory monitoring
def print_memory_usage():
    """Print current memory usage"""
    try:
        import psutil
        process = psutil.Process()
        memory_mb = process.memory_info().rss / (1024 * 1024)
        print(f"üíæ Current memory usage: {memory_mb:.1f} MB")
    except:
        pass

# Global start time
start_time = datetime.now()

# Main execution
def run_hybrid_training():
    """Run the complete hybrid training on ALL files"""
    print("üöÄ LSTM + XGBoost Hybrid Training")
    print("=" * 60)
    print("üéØ ALL FILES MODE - Processing EVERY gold file")
    print("=" * 60)

    print_memory_usage()

    try:
        trainer = HybridTrainer()
        trainer.run_hybrid_training(max_files=None)  # None = process all files

        total_time = datetime.now() - start_time
        print(f"\n‚úÖ All files training completed successfully!")
        print(f"‚è±Ô∏è Total training time: {total_time}")
        print(f"üìÅ All models saved to Google Drive")

    except Exception as e:
        print(f"‚ùå Hybrid training failed: {e}")
        import traceback
        traceback.print_exc()

# Run the hybrid training
if __name__ == "__main__":
    run_hybrid_training()